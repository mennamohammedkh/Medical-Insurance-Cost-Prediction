








import pandas as pd
import numpy as np

df = pd.read_csv("dataset/insurance.csv")
print("‚úÖ Dataset Loaded Successfully")





# ==========================================
# 3. Random Sample of Data
# ==========================================
print("\nüîπ Random Sample of Dataset:")
display(df.sample(5))





# ==========================================
# 4. Dataset Size
# ==========================================
rows, cols = df.shape
print(f"\nüîπ Dataset Size: {rows} rows, {cols} columns")





# ==========================================
# 5. Dataset Information
# ==========================================
print("\nüîπ Dataset Information:")
df.info()





# ==========================================
# 6. Column Names
# ==========================================
print("\nüîπ Column Names:")
print(list(df.columns))





# ==========================================
# 7. Statistical Description
# ==========================================
print("\nüîπ Statistical Summary:")
display(df.describe())





# ==========================================
# 8. Number of Unique Values per Column
# ==========================================
print("\nüîπ Unique Values per Column:")
display(df.nunique())





# ==========================================
# Data Balance Check ‚Äì Categorical Features
# ==========================================

categorical_columns = df.select_dtypes(include='object').columns

for col in categorical_columns:
    print(f"\nüîπ Distribution of {col}:")
    display(df[col].value_counts())
    print("Percentage Distribution:")
    display(df[col].value_counts(normalize=True) * 100)








# ==========================================
# Missing Values Check
# ==========================================
missing_values = df.isnull().sum()

print("üîπ Missing Values per Column:")
display(missing_values)





# ==========================================
# Imputation Strategy (If Needed)
# ==========================================

# Numerical ‚Üí Median (robust to outliers)
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns
df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())

# Categorical ‚Üí Mode
# ==========================================
# Safe Categorical Imputation (No Warning)
# ==========================================

categorical_cols = df.select_dtypes(include='object').columns

for col in categorical_cols:
    df[col] = df[col].fillna(df[col].mode()[0])









# ==========================================
# Duplicate Rows Detection
# ==========================================
duplicate_count = df.duplicated().sum()
print(f"üîπ Number of Duplicate Rows: {duplicate_count}")





# ==========================================
# Remove Duplicate Rows
# ==========================================
df.drop_duplicates(inplace=True)
print("‚úÖ Duplicate rows removed (if existed)")








# ==========================================
# Outlier Detection using IQR
# ==========================================

numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns

outlier_summary = {}

for col in numerical_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    outlier_summary[col] = len(outliers)

outlier_summary











# ==========================================
# Distribution of Numerical Features (Seaborn)
# ==========================================

import matplotlib.pyplot as plt
import seaborn as sns

numerical_features = ['age', 'bmi', 'children', 'charges']

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for i, feature in enumerate(numerical_features):
    sns.histplot(
        data=df,
        x=feature,
        kde=True,
        ax=axes[i]
    )
    axes[i].set_title(f'Distribution of {feature}', fontsize=14)
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel("Frequency")

plt.tight_layout()
plt.show()





# ==========================================
# Boxplots for Numerical Features
# ==========================================

import matplotlib.pyplot as plt
import seaborn as sns

numerical_features = ['age', 'bmi', 'children', 'charges']

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for i, feature in enumerate(numerical_features):
    sns.boxplot(
        x=df[feature],
        ax=axes[i],
        color='skyblue',
        flierprops=dict(marker='o', markerfacecolor='black', markersize=5)
    )
    axes[i].set_title(f'Boxplot of {feature}', fontsize=14)
    axes[i].set_xlabel(feature, fontsize=12)

plt.tight_layout()
plt.show()






# ==========================================
# Count Plots for Categorical Features
# ==========================================

import matplotlib.pyplot as plt
import seaborn as sns

categorical_features = ['sex', 'smoker', 'region']

fig, axes = plt.subplots(1, 3, figsize=(18, 6))

for i, feature in enumerate(categorical_features):
    sns.countplot(
        data=df,
        x=feature,
        ax=axes[i]
    )
    axes[i].set_title(f'Count Plot of {feature}', fontsize=14)
    axes[i].set_xlabel(feature, fontsize=12)
    axes[i].set_ylabel('Count', fontsize=12)

plt.tight_layout()
plt.show()





# ==========================================
# Skewness & Kurtosis
# ==========================================

skew_kurt_table = pd.DataFrame({
    "Feature": numerical_cols,
    "Skewness": df[numerical_cols].skew().values,
    "Kurtosis": df[numerical_cols].kurt().values
})

skew_kurt_table





# ==========================================
# Log Transformation Effect on Charges
# ==========================================

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Apply log transformation
df['charges_log'] = np.log1p(df['charges'])  # log(1 + x) ‚Üí numerically stable

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Original distribution
sns.histplot(
    df['charges'],
    kde=True,
    ax=axes[0]
)
axes[0].set_title(
    f'Original Charges\nSkewness: {df["charges"].skew():.2f}',
    fontsize=14,
    fontweight='bold'
)
axes[0].set_xlabel('Charges')

# Log-transformed distribution
sns.histplot(
    df['charges_log'],
    kde=True,
    ax=axes[1]
)
axes[1].set_title(
    f'Log-Transformed Charges\nSkewness: {df["charges_log"].skew():.2f}',
    fontsize=14,
    fontweight='bold'
)
axes[1].set_xlabel('Log(Charges + 1)')

plt.tight_layout()
plt.show()











# ==========================================
# Correlation Matrix
# ==========================================

numerical_features = ['age', 'bmi', 'children', 'charges']

corr_matrix = df[numerical_features].corr()
corr_matrix





import matplotlib.pyplot as plt
import seaborn as sns

numerical_features = ['age', 'bmi', 'children', 'charges']

# Correlation matrix
corr_matrix = df[numerical_features].corr()

# Heatmap
plt.figure(figsize=(10, 7))
sns.heatmap(
    corr_matrix, 
    annot=True, fmt=".2f", 
    cmap="coolwarm", 
    linewidths=0.5, 
    vmin=-1, vmax=1
)
plt.title("Correlation Heatmap of Numerical Features", fontsize=16, fontweight='bold')
plt.show()






features = ['age', 'bmi', 'children']

fig, axes = plt.subplots(1, 3, figsize=(20, 5))

for i, feature in enumerate(features):
    sns.scatterplot(
        data=df, 
        x=feature, 
        y='charges', 
        hue='smoker',  # üî• adds categorical effect
        palette={'yes':'red','no':'green'},
        alpha=0.6, ax=axes[i]
    )
    sns.regplot(
        data=df, x=feature, y='charges', scatter=False, ax=axes[i], color='blue'
    )
    axes[i].set_title(f'{feature.capitalize()} vs Charges', fontweight='bold')

plt.tight_layout()
plt.show()






categorical_features = ['sex', 'smoker', 'region']

fig, axes = plt.subplots(1, 3, figsize=(20, 6))

for i, feature in enumerate(categorical_features):
    sns.boxplot(
        data=df, 
        x=feature, 
        y='charges', 
        hue='smoker' if feature != 'smoker' else None, # add hue except for smoker itself
        palette='Set2', ax=axes[i]
    )
    axes[i].set_title(f'Charges Distribution by {feature.capitalize()}', fontweight='bold')

plt.tight_layout()
plt.show()





# Aggregation for multiple categorical features
agg_summary = df.groupby(['sex', 'smoker', 'region'])['charges'].agg(
    count='count',
    mean='mean',
    median='median',
    min='min',
    max='max'
).sort_values(by='mean', ascending=False)

agg_summary.head(10)  # top 10 for clarity






import numpy as np

df['charges_log'] = np.log1p(df['charges'])

# Compare log vs original by smoker
plt.figure(figsize=(12,6))
sns.kdeplot(df[df['smoker']=='yes']['charges_log'], label='Smoker', fill=True, color='red', alpha=0.5)
sns.kdeplot(df[df['smoker']=='no']['charges_log'], label='Non-Smoker', fill=True, color='green', alpha=0.5)
plt.title("Log-Transformed Charges Distribution by Smoker", fontsize=15, fontweight='bold')
plt.xlabel("Log(Charges + 1)")
plt.ylabel("Density")
plt.legend()
plt.show()






# Correlation with target
corr_target = df[numerical_features].corr()['charges'].sort_values(ascending=False)
corr_target_table = pd.DataFrame(corr_target).reset_index()
corr_target_table.columns = ['Feature', 'Correlation_with_Charges']
corr_target_table









import seaborn as sns
import matplotlib.pyplot as plt

numeric_features = ["age", "bmi", "children", "charges"]

sns.pairplot(df[numeric_features], diag_kind='kde')  # or diag_kind='hist'
plt.suptitle("Pairplot of Numerical Features", y=1.02, fontsize=16)

plt.show()





from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

ax.scatter(
    df['age'],
    df['bmi'],
    df['charges'],
    alpha=0.6
)

ax.set_xlabel('Age')
ax.set_ylabel('BMI')
ax.set_zlabel('Charges')
ax.set_title("3D Scatter: Age vs BMI vs Charges", fontweight='bold')

plt.show()








# ================================
# PCA - Exploratory Data Analysis
# ================================

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 1Ô∏è‚É£ Select numerical features (EXCLUDE target)
pca_features = ['age', 'bmi', 'children']
X_pca_input = df[pca_features]

# 2Ô∏è‚É£ Scale features (MANDATORY for PCA)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_pca_input)

# 3Ô∏è‚É£ Apply PCA (2 components for visualization)
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)

# 4Ô∏è‚É£ Create PCA DataFrame
pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])

# 5Ô∏è‚É£ Explained variance
explained_variance = pca.explained_variance_ratio_
print(f"Explained Variance Ratio:")
print(f"PC1: {explained_variance[0]:.2f}")
print(f"PC2: {explained_variance[1]:.2f}")
print(f"Total: {explained_variance.sum():.2f}")

# 6Ô∏è‚É£ PCA Scatter Plot
plt.figure(figsize=(8, 6))
plt.scatter(pca_df['PC1'], pca_df['PC2'], alpha=0.7)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Projection of Insurance Dataset (EDA)')
plt.grid(True)
plt.show()






from statsmodels.stats.outliers_influence import variance_inflation_factor

X_vif = df[numerical_features]
X_vif['Intercept'] = 1

vif_data = pd.DataFrame({
    "Feature": X_vif.columns,
    "VIF": [variance_inflation_factor(X_vif.values, i)
            for i in range(X_vif.shape[1])]
})

vif_data









from scipy.stats import zscore
import numpy as np

# Select numerical features
numerical_features = ['age', 'bmi', 'children', 'charges']

# Compute Z-scores
z_scores = np.abs(zscore(df[numerical_features]))

# Threshold
z_threshold = 3

# Identify outliers
outliers_z = (z_scores > z_threshold).any(axis=1)

print(f"üîπ Number of outliers detected using Z-score: {outliers_z.sum()}")

#=================================================

outliers_iqr = pd.Series(False, index=df.index)

for col in numerical_features:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers_iqr |= (df[col] < lower_bound) | (df[col] > upper_bound)
 
print(f"üîπ Number of outliers detected using IQR: {outliers_iqr.sum()}")





df_clean = df.copy()

for col in numerical_features:
    Q1 = df_clean[col].quantile(0.25)
    Q3 = df_clean[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    df_clean[col] = df_clean[col].clip(lower_bound, upper_bound)





import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

for i, col in enumerate(numerical_features):
    axes[i//2, i%2].boxplot(df_clean[col], vert=False)
    axes[i//2, i%2].set_title(f'Boxplot after Outlier Treatment: {col}')

plt.tight_layout()
plt.show()





print("Invalid age values:", (df_clean['age'] < 0).sum())
print("Invalid BMI values:", (df_clean['bmi'] <= 0).sum())
print("Invalid children values:", (df_clean['children'] < 0).sum())
print("Invalid charges values:", (df_clean['charges'] <= 0).sum())





threshold = 0.05

for col in categorical_features:
    rare_categories = df_clean[col].value_counts(normalize=True)
    rare_categories = rare_categories[rare_categories < threshold]

    if not rare_categories.empty:
        print(f"\nRare categories in {col}:")
        print(rare_categories)











import pandas as pd

# Select numerical features + target
numerical_features = ['age', 'bmi', 'children', 'charges']

# Correlation matrix
corr_matrix = df_clean[numerical_features].corr()

# Correlation with target
target_corr = corr_matrix['charges'].drop('charges').sort_values(ascending=False)

print("üîπ Correlation with Charges:")
print(target_corr)





from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import OneHotEncoder

# Encode categorical features
df_encoded = pd.get_dummies(df_clean, drop_first=True)

X = df_encoded.drop('charges', axis=1)
y = df_encoded['charges']

rf = RandomForestRegressor(random_state=42)
rf.fit(X, y)

feature_importance = pd.Series(
    rf.feature_importances_, index=X.columns
).sort_values(ascending=False)

print(feature_importance.head(10))
print("‚ö†Ô∏è Use this only for insight, not final conclusion.")








!pip install ydata-profiling


from ydata_profiling import ProfileReport

profile = ProfileReport(df_clean, title="Insurance Dataset EDA Report")
profile











import matplotlib.pyplot as plt

# Ensure df_clean exists
df_clean = df.copy()

fig, axes = plt.subplots(1, 2, figsize=(14,5))

# Charges vs BMI
axes[0].scatter(df_clean['bmi'], df_clean['charges'], alpha=0.5)
axes[0].set_xlabel('BMI')
axes[0].set_ylabel('Charges')
axes[0].set_title('Charges vs BMI')

# Charges vs Age
axes[1].scatter(df_clean['age'], df_clean['charges'], alpha=0.5)
axes[1].set_xlabel('Age')
axes[1].set_ylabel('Charges')
axes[1].set_title('Charges vs Age')

plt.tight_layout()
plt.show()








from sklearn.preprocessing import OneHotEncoder

# Separate features and target
X = df_clean.drop('charges', axis=1)
y = df_clean['charges']

# Identify categorical and numerical columns
categorical_cols = ['sex', 'smoker', 'region']
numerical_cols = ['age', 'bmi', 'children']

# One-hot encode categorical variables
X_encoded = pd.get_dummies(
    X,
    columns=categorical_cols,
    drop_first=True
)

print("üîπ Encoded feature shape:", X_encoded.shape)





import matplotlib.pyplot as plt
import seaborn as sns

categorical_cols = ['sex', 'smoker', 'region']

plt.figure(figsize=(15,4))

for i, col in enumerate(categorical_cols):
    plt.subplot(1, len(categorical_cols), i+1)
    sns.countplot(x=col, data=df_clean, palette='Set2')
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Count')

plt.tight_layout()
plt.show()





import pandas as pd

# One-hot encoding
X_encoded = pd.get_dummies(df_clean.drop('charges', axis=1), drop_first=True)

# Combine with target
df_encoded = pd.concat([X_encoded, df_clean['charges']], axis=1)

# Correlation heatmap
plt.figure(figsize=(10,6))
sns.heatmap(df_encoded.corr()['charges'].sort_values(ascending=False).to_frame(), 
            annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title("Correlation of Features with Target (Charges)")
plt.show()





sns.pairplot(df_clean, hue='smoker', vars=['age', 'bmi', 'children', 'charges'])
plt.show()





### -Scaling is required for:

####- Linear Regression (coefficients stability)
####- Regularization (Ridge / Lasso)
####- Distance-based methods


"""from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_encoded[numerical_cols] = scaler.fit_transform(
    X_encoded[numerical_cols]
)"""











import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# 1Ô∏è‚É£ Visualize target distribution
plt.figure(figsize=(8,5))
sns.histplot(df['charges'], kde=True, bins=30, color='skyblue')
plt.title(" Distribution of Target Variable: Charges")
plt.xlabel("Charges")
plt.ylabel("Frequency")
plt.show()

# 2Ô∏è‚É£ Check skewness
skewness = df['charges'].skew()
print(f"Skewness of charges: {skewness:.2f}")

# 3Ô∏è‚É£ Apply log transformation if highly skewed
if abs(skewness) > 1:
    df['charges'] = np.log1p(df['charges'])  # log(1 + x) to handle zeros
    print("üîπ Applied log transformation to 'charges' for better distribution.")

# 4Ô∏è‚É£ Re-visualize transformed target (if transformed)
plt.figure(figsize=(8,5))
sns.histplot(df['charges'], kde=True, bins=30, color='salmon')
plt.title(" Distribution of Target Variable after Transformation")
plt.xlabel("Charges")
plt.ylabel("Frequency")
plt.show()





# Target
y = df_clean['charges']                    # what we predict

# Features (drop target)
X = df_clean.drop(columns=['charges'])      # what we use to predict






from sklearn.model_selection import train_test_split

# X = features after encoding and scaling
# y = target variable ('charges')

X_train, X_test, y_train, y_test = train_test_split(
    X_encoded,   # or your scaled feature dataframe
    y, 
    test_size=0.2,   # 20% test, 80% train
    random_state=42  # for reproducibility
)

print("‚úÖ Train-Test split done")
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)






from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


print("‚úÖ Encoding ‚Üí Split ‚Üí Scaling completed successfully")











import matplotlib.pyplot as plt
import seaborn as sns

# Plot numeric features vs target
numeric_features = ['age', 'bmi', 'children']

plt.figure(figsize=(15,4))
for i, col in enumerate(numeric_features):
    plt.subplot(1, len(numeric_features), i+1)
    sns.scatterplot(x=col, y='charges', data=df_clean)
    plt.title(f'{col} vs Charges')
plt.tight_layout()
plt.show()





# Pearson correlation for numeric features vs target
corr = df_clean[numeric_features + ['charges']].corr()['charges'].drop('charges')
print(corr)


from sklearn.linear_model import LinearRegression

X = df_clean[numeric_features]
y = df_clean['charges']

model = LinearRegression()
model.fit(X, y)
y_pred = model.predict(X)

residuals = y - y_pred

plt.figure(figsize=(6,4))
sns.scatterplot(x=y_pred, y=residuals)
plt.axhline(0, color='red', linestyle='--')
plt.title('Residual Plot')
plt.xlabel('Predicted charges')
plt.ylabel('Residuals')
plt.show()



